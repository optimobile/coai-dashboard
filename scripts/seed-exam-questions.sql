-- Seed exam questions for CSOAI certification exam
-- 50 questions covering all topics

-- AI Safety Fundamentals (10 questions - 20%)
INSERT INTO exam_questions (category, question, optionA, optionB, optionC, optionD, correctAnswer, explanation, difficulty, points) VALUES
('AI Safety Fundamentals', 'What is the primary purpose of AI safety oversight?', 'To slow down AI development', 'To ensure AI systems operate safely and ethically', 'To replace human workers', 'To increase AI profits', 'B', 'AI safety oversight ensures AI systems operate safely, ethically, and in alignment with human values.', 'easy', 1),
('AI Safety Fundamentals', 'Which of the following is NOT a key principle of responsible AI?', 'Transparency', 'Accountability', 'Profitability maximization', 'Fairness', 'C', 'Responsible AI principles include transparency, accountability, fairness, and safety - not profit maximization.', 'easy', 1),
('AI Safety Fundamentals', 'What is "AI alignment" in the context of AI safety?', 'Aligning AI code with programming standards', 'Ensuring AI systems act in accordance with human values and intentions', 'Aligning AI hardware components', 'Aligning AI companies with regulations', 'B', 'AI alignment refers to ensuring AI systems behave in ways that are aligned with human values, intentions, and goals.', 'medium', 1),
('AI Safety Fundamentals', 'What is a "black box" problem in AI?', 'AI systems that are painted black', 'AI systems whose decision-making processes are not transparent or explainable', 'AI systems that only work in dark environments', 'AI systems that store data in black boxes', 'B', 'The black box problem refers to AI systems whose internal decision-making processes are opaque and difficult to understand.', 'medium', 1),
('AI Safety Fundamentals', 'Which organization is primarily responsible for AI safety standards in the United States?', 'FDA', 'NIST', 'EPA', 'FCC', 'B', 'NIST (National Institute of Standards and Technology) leads AI safety standards development in the US through the AI RMF.', 'easy', 1),
('AI Safety Fundamentals', 'What is the role of a Watchdog Analyst in AI safety?', 'To develop new AI systems', 'To monitor, review, and report on AI system safety and compliance', 'To sell AI products', 'To replace AI systems', 'B', 'Watchdog Analysts monitor AI systems for safety issues, review compliance, and report findings to ensure public protection.', 'easy', 1),
('AI Safety Fundamentals', 'What is "adversarial AI"?', 'AI systems that compete in games', 'Techniques used to fool or manipulate AI systems', 'AI systems that argue with users', 'AI systems from competing companies', 'B', 'Adversarial AI refers to techniques designed to deceive or manipulate AI systems, often through specially crafted inputs.', 'medium', 1),
('AI Safety Fundamentals', 'What is the significance of the EU AI Act enforcement date of February 2, 2026?', 'It marks when AI becomes illegal', 'It is when compliance requirements for high-risk AI systems take effect', 'It is when all AI must be shut down', 'It is a voluntary guideline date', 'B', 'February 2, 2026 is when EU AI Act compliance requirements for high-risk AI systems become mandatory.', 'easy', 1),
('AI Safety Fundamentals', 'What is "human-in-the-loop" in AI systems?', 'A human physically inside a computer', 'Human oversight and intervention capability in AI decision-making', 'A type of AI training data', 'A programming loop written by humans', 'B', 'Human-in-the-loop refers to systems where humans can oversee, intervene in, or override AI decisions.', 'easy', 1),
('AI Safety Fundamentals', 'Why is documentation important in AI safety?', 'It makes AI systems run faster', 'It provides transparency, accountability, and enables auditing', 'It is only needed for marketing', 'It is not important', 'B', 'Documentation is crucial for transparency, accountability, regulatory compliance, and enabling effective audits.', 'easy', 1);

-- EU AI Act (12 questions - 25%)
INSERT INTO exam_questions (category, question, optionA, optionB, optionC, optionD, correctAnswer, explanation, difficulty, points) VALUES
('EU AI Act', 'Which of the following AI systems would be classified as "high-risk" under the EU AI Act?', 'An AI-powered spam filter for email', 'A recommendation system for music playlists', 'An AI system used for credit scoring decisions', 'A chatbot for customer service inquiries', 'C', 'Credit scoring AI systems are classified as high-risk because they significantly impact individuals financial access.', 'medium', 1),
('EU AI Act', 'What is the maximum penalty for violations of prohibited AI practices under the EU AI Act?', '€7.5 million or 1.5% of global turnover', '€15 million or 3% of global turnover', '€35 million or 7% of global turnover', '€50 million or 10% of global turnover', 'C', 'Violations of prohibited AI practices can result in fines up to €35 million or 7% of global annual turnover.', 'medium', 1),
('EU AI Act', 'Which AI practice is explicitly prohibited under the EU AI Act?', 'Facial recognition for law enforcement', 'Social scoring by governments', 'Medical diagnosis AI', 'Autonomous vehicles', 'B', 'Social scoring systems by public authorities that evaluate citizens based on behavior are explicitly prohibited.', 'medium', 1),
('EU AI Act', 'What are the four risk categories under the EU AI Act?', 'Low, Medium, High, Critical', 'Unacceptable, High, Limited, Minimal', 'Safe, Moderate, Dangerous, Extreme', 'Green, Yellow, Orange, Red', 'B', 'The EU AI Act categorizes AI systems as Unacceptable (banned), High-risk, Limited risk, or Minimal risk.', 'easy', 1),
('EU AI Act', 'Which of the following is required for high-risk AI systems under the EU AI Act?', 'Only a user manual', 'Risk management system, data governance, and technical documentation', 'Just a privacy policy', 'No specific requirements', 'B', 'High-risk AI systems require comprehensive risk management, data governance, technical documentation, and human oversight.', 'medium', 1),
('EU AI Act', 'What is the purpose of the EU AI Act conformity assessment?', 'To certify AI developers', 'To verify that high-risk AI systems meet regulatory requirements before market placement', 'To test AI speed', 'To compare AI prices', 'B', 'Conformity assessment verifies that high-risk AI systems comply with EU AI Act requirements before being placed on the market.', 'medium', 1),
('EU AI Act', 'Under the EU AI Act, who is primarily responsible for ensuring AI system compliance?', 'End users only', 'The European Commission', 'Providers (developers) of AI systems', 'Internet service providers', 'C', 'Providers (developers) bear primary responsibility for ensuring their AI systems comply with EU AI Act requirements.', 'easy', 1),
('EU AI Act', 'What transparency obligation applies to AI systems that interact with humans under the EU AI Act?', 'No transparency required', 'Users must be informed they are interacting with an AI system', 'Only written disclosure in terms of service', 'Transparency is optional', 'B', 'AI systems interacting with humans must clearly inform users that they are interacting with an AI, not a human.', 'easy', 1),
('EU AI Act', 'Which sector is NOT specifically mentioned as high-risk in the EU AI Act?', 'Education and vocational training', 'Employment and worker management', 'Entertainment and gaming', 'Law enforcement', 'C', 'Entertainment and gaming are generally not classified as high-risk sectors under the EU AI Act.', 'medium', 1),
('EU AI Act', 'What is required for AI systems used in biometric identification?', 'No special requirements', 'Prior authorization and strict safeguards', 'Only user consent', 'Registration with local police', 'B', 'Real-time biometric identification in public spaces requires prior judicial or administrative authorization and strict safeguards.', 'hard', 1),
('EU AI Act', 'How often must high-risk AI systems undergo monitoring and review?', 'Never after initial deployment', 'Throughout their entire lifecycle', 'Only when problems occur', 'Every 5 years', 'B', 'High-risk AI systems must be continuously monitored and reviewed throughout their entire operational lifecycle.', 'medium', 1),
('EU AI Act', 'What is the role of national competent authorities under the EU AI Act?', 'To develop AI systems', 'To supervise and enforce AI Act compliance in their jurisdiction', 'To sell AI products', 'To train AI developers', 'B', 'National competent authorities are responsible for supervising the market and enforcing EU AI Act compliance.', 'easy', 1);

-- NIST AI RMF (12 questions - 25%)
INSERT INTO exam_questions (category, question, optionA, optionB, optionC, optionD, correctAnswer, explanation, difficulty, points) VALUES
('NIST AI RMF', 'What are the four core functions of the NIST AI Risk Management Framework?', 'Plan, Do, Check, Act', 'Govern, Map, Measure, Manage', 'Identify, Protect, Detect, Respond', 'Assess, Design, Implement, Monitor', 'B', 'The NIST AI RMF consists of four core functions: Govern, Map, Measure, and Manage.', 'easy', 1),
('NIST AI RMF', 'What is the purpose of the "Govern" function in NIST AI RMF?', 'To control AI system access', 'To establish organizational AI risk management culture, policies, and accountability', 'To govern AI data storage', 'To manage AI budgets', 'B', 'The Govern function establishes organizational culture, policies, processes, and accountability for AI risk management.', 'medium', 1),
('NIST AI RMF', 'What does the "Map" function in NIST AI RMF involve?', 'Creating geographic maps of AI deployment', 'Identifying and documenting AI system contexts, capabilities, and potential impacts', 'Mapping network connections', 'Creating user journey maps', 'B', 'The Map function involves understanding the AI system context, identifying risks, and documenting potential impacts.', 'medium', 1),
('NIST AI RMF', 'What is measured in the "Measure" function of NIST AI RMF?', 'AI system performance speed', 'AI risks using quantitative and qualitative methods', 'AI system physical dimensions', 'AI development costs', 'B', 'The Measure function assesses and analyzes AI risks using both quantitative and qualitative methods.', 'medium', 1),
('NIST AI RMF', 'What activities are included in the "Manage" function of NIST AI RMF?', 'Managing AI team schedules', 'Prioritizing, responding to, and monitoring AI risks', 'Managing AI hardware inventory', 'Managing AI marketing campaigns', 'B', 'The Manage function involves prioritizing risks, implementing responses, and continuously monitoring AI systems.', 'medium', 1),
('NIST AI RMF', 'Is the NIST AI RMF mandatory for US companies?', 'Yes, for all companies', 'No, it is a voluntary framework', 'Only for government contractors', 'Only for large corporations', 'B', 'The NIST AI RMF is a voluntary framework, though it may be referenced in contracts or regulations.', 'easy', 1),
('NIST AI RMF', 'What is "AI trustworthiness" according to NIST?', 'Whether AI can be trusted with passwords', 'Characteristics including validity, reliability, safety, security, and fairness', 'How much money AI systems cost', 'How fast AI systems operate', 'B', 'NIST defines AI trustworthiness through characteristics like validity, reliability, safety, security, fairness, and transparency.', 'medium', 1),
('NIST AI RMF', 'How does NIST AI RMF relate to other risk management frameworks?', 'It replaces all other frameworks', 'It is designed to complement and integrate with existing risk management practices', 'It conflicts with other frameworks', 'It only works standalone', 'B', 'NIST AI RMF is designed to complement and integrate with existing organizational risk management frameworks.', 'medium', 1),
('NIST AI RMF', 'What stakeholders should be involved in AI risk management according to NIST?', 'Only AI developers', 'Diverse stakeholders including affected communities, domain experts, and leadership', 'Only executive management', 'Only IT departments', 'B', 'NIST recommends involving diverse stakeholders including affected communities, domain experts, and organizational leadership.', 'medium', 1),
('NIST AI RMF', 'What is the relationship between AI risk and AI impact in NIST AI RMF?', 'They are unrelated', 'Higher potential impact generally correlates with higher risk requiring more rigorous management', 'Lower impact means higher risk', 'Impact is not considered', 'B', 'NIST AI RMF recognizes that higher potential impacts generally require more rigorous risk management approaches.', 'medium', 1),
('NIST AI RMF', 'What does NIST recommend for AI systems with emergent or unpredictable behavior?', 'Deploy them immediately', 'Enhanced monitoring, testing, and human oversight', 'Ignore the behavior', 'Remove all safeguards', 'B', 'NIST recommends enhanced monitoring, testing, and human oversight for AI systems with emergent or unpredictable behaviors.', 'hard', 1),
('NIST AI RMF', 'How should organizations document AI risk management activities according to NIST?', 'Documentation is optional', 'Maintain comprehensive records of decisions, assessments, and actions', 'Only document failures', 'Use verbal communication only', 'B', 'NIST recommends maintaining comprehensive documentation of all risk management decisions, assessments, and actions.', 'easy', 1);

-- AI Bias & Fairness (8 questions - 15%)
INSERT INTO exam_questions (category, question, optionA, optionB, optionC, optionD, correctAnswer, explanation, difficulty, points) VALUES
('AI Bias & Fairness', 'Which type of bias occurs when training data does not represent the population the AI will serve?', 'Algorithmic bias', 'Data bias', 'Deployment bias', 'Confirmation bias', 'B', 'Data bias occurs when training data is not representative of the population the AI system will actually serve.', 'easy', 1),
('AI Bias & Fairness', 'What is "algorithmic bias"?', 'Bias in computer hardware', 'Systematic errors in AI outputs that create unfair outcomes for certain groups', 'Preference for certain programming languages', 'Bias in algorithm naming', 'B', 'Algorithmic bias refers to systematic errors in AI outputs that create unfair or discriminatory outcomes for certain groups.', 'easy', 1),
('AI Bias & Fairness', 'What is "deployment bias" in AI systems?', 'Bias in how AI is marketed', 'Bias that emerges when AI is used in contexts different from its training environment', 'Bias in deployment schedules', 'Bias in server locations', 'B', 'Deployment bias occurs when AI systems are used in contexts or populations different from those they were trained on.', 'medium', 1),
('AI Bias & Fairness', 'Which of the following is a common source of bias in AI training data?', 'Random number generators', 'Historical human decisions that reflected societal biases', 'Computer processing speed', 'Internet connection quality', 'B', 'Historical data often reflects past societal biases, which can be learned and perpetuated by AI systems.', 'easy', 1),
('AI Bias & Fairness', 'What is "fairness through awareness" in AI?', 'Making AI systems aware of their surroundings', 'Explicitly considering protected attributes to ensure equitable outcomes', 'Training AI to be polite', 'Making users aware of AI', 'B', 'Fairness through awareness involves explicitly considering protected attributes to ensure equitable treatment across groups.', 'hard', 1),
('AI Bias & Fairness', 'What is "disparate impact" in the context of AI fairness?', 'Different AI systems having different speeds', 'When a seemingly neutral AI practice disproportionately affects protected groups', 'Impact of AI on different industries', 'Different impacts of AI on profits', 'B', 'Disparate impact occurs when a neutral-appearing AI practice has disproportionately negative effects on protected groups.', 'medium', 1),
('AI Bias & Fairness', 'How can organizations detect bias in their AI systems?', 'By ignoring user complaints', 'Through regular auditing, testing across demographic groups, and monitoring outcomes', 'By only testing on majority populations', 'Bias cannot be detected', 'B', 'Bias detection requires regular auditing, testing across different demographic groups, and continuous outcome monitoring.', 'medium', 1),
('AI Bias & Fairness', 'What is the challenge of achieving "individual fairness" in AI?', 'It is too expensive', 'Defining and measuring similarity between individuals is complex and context-dependent', 'It requires too much data', 'Individual fairness is not important', 'B', 'Individual fairness requires treating similar individuals similarly, but defining similarity is complex and context-dependent.', 'hard', 1);

-- Analyst Decision-Making (8 questions - 15%)
INSERT INTO exam_questions (category, question, optionA, optionB, optionC, optionD, correctAnswer, explanation, difficulty, points) VALUES
('Analyst Decision-Making', 'What is the role of the 33-Agent Byzantine Council in CSOAI?', 'To develop new AI systems', 'To provide impartial, consensus-based decisions on AI safety matters', 'To sell AI products', 'To train AI models', 'B', 'The 33-Agent Byzantine Council provides vendor-independent, impartial decisions through multi-perspective consensus.', 'easy', 1),
('Analyst Decision-Making', 'What does "Byzantine fault tolerance" mean in the context of the Council?', 'Tolerance for Byzantine architecture', 'The system can reach correct consensus even if some agents are faulty or malicious', 'Tolerance for old systems', 'Tolerance for slow processing', 'B', 'Byzantine fault tolerance ensures the Council can reach correct consensus even if some agents provide incorrect or malicious inputs.', 'medium', 1),
('Analyst Decision-Making', 'What should a Watchdog Analyst do when they identify a potential AI safety issue?', 'Ignore it if it seems minor', 'Document the issue thoroughly and submit a formal report', 'Post about it on social media', 'Contact the AI company directly', 'B', 'Analysts should document issues thoroughly and submit formal reports through proper channels for review and action.', 'easy', 1),
('Analyst Decision-Making', 'What information should be included in a Watchdog report?', 'Only the analysts opinion', 'Factual description, evidence, impact assessment, and recommended actions', 'Just a brief summary', 'Only positive findings', 'B', 'Reports should include factual descriptions, supporting evidence, impact assessments, and recommended actions.', 'easy', 1),
('Analyst Decision-Making', 'How should an analyst handle conflicts of interest?', 'Ignore them', 'Disclose them and recuse from relevant decisions', 'Hide them from supervisors', 'Conflicts are not possible', 'B', 'Analysts must disclose conflicts of interest and recuse themselves from decisions where conflicts exist.', 'easy', 1),
('Analyst Decision-Making', 'What is the SOAI-PDCA cycle?', 'A type of AI model', 'Safety Of AI - Plan, Do, Check, Act continuous improvement methodology', 'A programming language', 'A certification exam', 'B', 'SOAI-PDCA is the continuous improvement methodology adapted for AI safety governance.', 'easy', 1),
('Analyst Decision-Making', 'When should an analyst escalate an issue to the Council?', 'Never', 'When the issue is significant, complex, or requires multi-perspective review', 'For every minor issue', 'Only when asked by management', 'B', 'Issues should be escalated to the Council when they are significant, complex, or benefit from multi-perspective review.', 'medium', 1),
('Analyst Decision-Making', 'What is the importance of maintaining objectivity as a Watchdog Analyst?', 'It is not important', 'It ensures fair, unbiased assessments that protect public trust', 'It makes work easier', 'It is only important for senior analysts', 'B', 'Objectivity is essential for fair, unbiased assessments that maintain public trust in the oversight system.', 'easy', 1);
