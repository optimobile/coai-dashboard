# CSOAI - Council of Safety of AI
# robots.txt - Search Engine Crawling Rules

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Disallow private/admin areas
Disallow: /admin/
Disallow: /api/
Disallow: /dashboard/private/
Disallow: /settings/

# Disallow sensitive pages
Disallow: /auth/
Disallow: /login
Disallow: /signup
Disallow: /password-reset
Disallow: /verify-email

# Disallow temporary pages
Disallow: /temp/
Disallow: /test/
Disallow: /staging/

# Disallow search results
Disallow: /search?
Disallow: /results?

# Disallow duplicate content
Disallow: /*?sort=
Disallow: /*?filter=
Disallow: /*?page=2

# Sitemap location
Sitemap: https://councilof.ai/sitemap.xml

# Crawl delay (optional - helps with server load)
Crawl-delay: 1

# Request rate (optional)
Request-rate: 30/60

# Specific rules for Google
User-agent: Googlebot
Allow: /
Crawl-delay: 0

# Specific rules for Bing
User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Block bad bots
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: MJ12bot
Disallow: /

# Allow specific bots
User-agent: Slurp
Allow: /

User-agent: Slorp
Allow: /
