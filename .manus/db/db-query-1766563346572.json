{
  "query": "INSERT INTO test_questions (testId, questionText, questionType, options, correctAnswer, explanation, points, difficulty, isActive) VALUES \n(1, 'Under China''s AI regulations, which type of AI service requires algorithm registration with the Cyberspace Administration?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"All AI services\"},{\"id\":\"B\",\"text\":\"Only government AI systems\"},{\"id\":\"C\",\"text\":\"Recommendation algorithms and generative AI services\"},{\"id\":\"D\",\"text\":\"Only autonomous vehicles\"}]', 'C', 'China requires algorithm registration for recommendation algorithms (under the Algorithm Recommendation Regulations) and generative AI services (under the Generative AI Measures) with the Cyberspace Administration of China.', 1, 'medium', true),\n(1, 'Which principle is emphasized in both EU AI Act and TC260 frameworks?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"AI development should be unrestricted\"},{\"id\":\"B\",\"text\":\"Human oversight and control of AI systems\"},{\"id\":\"C\",\"text\":\"AI should replace human decision-making entirely\"},{\"id\":\"D\",\"text\":\"AI development should be government-controlled\"}]', 'B', 'Both the EU AI Act and TC260 framework emphasize the importance of human oversight and maintaining human control over AI systems, though they differ in implementation approaches.', 1, 'easy', true),\n(1, 'What is ''algorithmic bias'' in AI systems?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"Intentional discrimination programmed by developers\"},{\"id\":\"B\",\"text\":\"Systematic errors that create unfair outcomes for certain groups\"},{\"id\":\"C\",\"text\":\"Preference for certain programming languages\"},{\"id\":\"D\",\"text\":\"Hardware limitations affecting performance\"}]', 'B', 'Algorithmic bias refers to systematic and repeatable errors in AI systems that create unfair outcomes, such as privileging one group over another. It can arise from biased training data, flawed algorithms, or inappropriate use.', 1, 'easy', true),\n(1, 'Which of the following is an example of ''proxy discrimination'' in AI?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"Using race directly as a feature in a model\"},{\"id\":\"B\",\"text\":\"Using zip code as a feature that correlates with race\"},{\"id\":\"C\",\"text\":\"Removing all demographic data from training\"},{\"id\":\"D\",\"text\":\"Using random sampling for training data\"}]', 'B', 'Proxy discrimination occurs when seemingly neutral features (like zip code) serve as proxies for protected characteristics (like race) because they are highly correlated, leading to discriminatory outcomes even without explicit use of protected attributes.', 1, 'medium', true),\n(1, 'What is ''disparate impact'' in the context of AI systems?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"When AI systems crash unexpectedly\"},{\"id\":\"B\",\"text\":\"When a neutral policy disproportionately affects a protected group\"},{\"id\":\"C\",\"text\":\"When AI systems have different performance on different hardware\"},{\"id\":\"D\",\"text\":\"When AI systems are deployed in different regions\"}]', 'B', 'Disparate impact occurs when a facially neutral policy or practice disproportionately affects members of a protected group, even without discriminatory intent. In AI, this often manifests as different error rates across demographic groups.', 1, 'medium', true);",
  "command": "mysql --batch --raw --column-names --default-character-set=utf8mb4 --host gateway02.us-east-1.prod.aws.tidbcloud.com --port 4000 --user 3VC7xi4RHX81jG7.65112bc774d1 --database K34VNBtbDJyjtW8qhrCBdB --execute INSERT INTO test_questions (testId, questionText, questionType, options, correctAnswer, explanation, points, difficulty, isActive) VALUES \n(1, 'Under China''s AI regulations, which type of AI service requires algorithm registration with the Cyberspace Administration?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"All AI services\"},{\"id\":\"B\",\"text\":\"Only government AI systems\"},{\"id\":\"C\",\"text\":\"Recommendation algorithms and generative AI services\"},{\"id\":\"D\",\"text\":\"Only autonomous vehicles\"}]', 'C', 'China requires algorithm registration for recommendation algorithms (under the Algorithm Recommendation Regulations) and generative AI services (under the Generative AI Measures) with the Cyberspace Administration of China.', 1, 'medium', true),\n(1, 'Which principle is emphasized in both EU AI Act and TC260 frameworks?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"AI development should be unrestricted\"},{\"id\":\"B\",\"text\":\"Human oversight and control of AI systems\"},{\"id\":\"C\",\"text\":\"AI should replace human decision-making entirely\"},{\"id\":\"D\",\"text\":\"AI development should be government-controlled\"}]', 'B', 'Both the EU AI Act and TC260 framework emphasize the importance of human oversight and maintaining human control over AI systems, though they differ in implementation approaches.', 1, 'easy', true),\n(1, 'What is ''algorithmic bias'' in AI systems?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"Intentional discrimination programmed by developers\"},{\"id\":\"B\",\"text\":\"Systematic errors that create unfair outcomes for certain groups\"},{\"id\":\"C\",\"text\":\"Preference for certain programming languages\"},{\"id\":\"D\",\"text\":\"Hardware limitations affecting performance\"}]', 'B', 'Algorithmic bias refers to systematic and repeatable errors in AI systems that create unfair outcomes, such as privileging one group over another. It can arise from biased training data, flawed algorithms, or inappropriate use.', 1, 'easy', true),\n(1, 'Which of the following is an example of ''proxy discrimination'' in AI?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"Using race directly as a feature in a model\"},{\"id\":\"B\",\"text\":\"Using zip code as a feature that correlates with race\"},{\"id\":\"C\",\"text\":\"Removing all demographic data from training\"},{\"id\":\"D\",\"text\":\"Using random sampling for training data\"}]', 'B', 'Proxy discrimination occurs when seemingly neutral features (like zip code) serve as proxies for protected characteristics (like race) because they are highly correlated, leading to discriminatory outcomes even without explicit use of protected attributes.', 1, 'medium', true),\n(1, 'What is ''disparate impact'' in the context of AI systems?', 'multiple_choice', '[{\"id\":\"A\",\"text\":\"When AI systems crash unexpectedly\"},{\"id\":\"B\",\"text\":\"When a neutral policy disproportionately affects a protected group\"},{\"id\":\"C\",\"text\":\"When AI systems have different performance on different hardware\"},{\"id\":\"D\",\"text\":\"When AI systems are deployed in different regions\"}]', 'B', 'Disparate impact occurs when a facially neutral policy or practice disproportionately affects members of a protected group, even without discriminatory intent. In AI, this often manifests as different error rates across demographic groups.', 1, 'medium', true);",
  "rows": [],
  "messages": [],
  "stdout": "",
  "stderr": "",
  "execution_time_ms": 59
}